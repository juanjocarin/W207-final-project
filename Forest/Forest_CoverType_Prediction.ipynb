{"nbformat_minor": 0, "cells": [{"source": "#Forest Cover Type Prediction\n\nSubmitted by :\nJuanjo Carin,Vamsi Sakhamuri,Tuhin Mahmud\n\nDate: July 16, 2015\n\n<img src=\"https://kaggle2.blob.core.windows.net/competitions/kaggle/3936/logos/front_page.png\">\n\nKaggle Competition hosted at https://www.kaggle.com/c/forest-cover-type-prediction\n", "cell_type": "markdown", "metadata": {}}, {"source": "## Description\n### Use cartographic variables to classify forest categories\nRandom forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing.\n\nIn this competition you are asked to predict the forest cover type (the predominant kind of tree cover) \nfrom strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type \nfor a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information \nSystem data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. \nThe data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables \nsuch as wilderness areas and soil type.\n\nThis study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. \nThese areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more\na result of ecological processes rather than forest management practices.\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "# This tells matplotlib not to try opening a new window for each plot.\n%matplotlib inline\n\n# General libraries.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# SK-learn libraries for learning.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.grid_search import GridSearchCV\n\n# SK-learn libraries for evaluation.\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"source": "Let's talk about how to enter a kaggle competition.\nFrist you sign up for kaggle.\n\nhttps://www.kaggle.com\n\nthen, you find a competition. We'll use the MNIST digit data set!\n\nhttps://www.kaggle.com/c/digit-recognizer\n\nWe now have a couple of csv files, test.csv, and train.csv\n\nFind where they live on your file system, and now we need to load the data:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "ff = \"train.csv\" # you will need to edit this directory\nf = open(ff)\ncolumn_names = f.readline() # you'd needs this ordinarily\n\ndata = np.loadtxt(f, delimiter=\",\")\n\ny, X = data[:, -1], data[:, :-1]\n\nff_test = \"test.csv\" # you will need to edit this directory\nf_test = open(ff_test)\ncolumn_names_test = f_test.readline() # you'd needs this ordinarily\n\ndata_test = np.loadtxt(f_test, delimiter=\",\")\n\n# note there are no labels here!\nX_test = data_test\n\nprint X_test.shape\nprint X.shape\nprint y.shape", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(565892L, 55L)\n(15120L, 55L)\n(15120L,)\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 3, "cell_type": "code", "source": "print y", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[ 5.  5.  2. ...,  3.  3.  3.]\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Now, we train a model, let's just do KNN, no time to think!", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "# to make this faster, just use the first 1000\ny_train, X_train = y[:1000], X[:1000, :]\n\nkn = KNeighborsClassifier(n_neighbors=1)\n\nkn.fit(X_train, y_train)\n\n# here's what we need to send back to Kaggle\npreds = kn.predict(X_test)\nprint preds", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[ 1.  1.  1. ...,  1.  1.  1.]\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Now we need to save the output to a textfile, and upload the results to kaggle!\n\nRead the data page to make you submission the right format:\n\nhttps://www.kaggle.com/c/digit-recognizer/data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "test_lab_f = open(\"test_labeled.csv\", \"w\") # you will need to edit this directory\n\ntest_lab_f.write(\"Id,Cover_Type\")\n\nidx = X.shape[0]\n                 \nfor pp in preds:\n    idx += 1\n    test_lab_f.write(\"\\n\")\n    test_lab_f.write(str(idx) + \",\" + str(int(pp)))\n   \n\ntest_lab_f.close()", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "## Results\n* July 11, 2015 - Simple KNN testing on the data   \n\n", "cell_type": "markdown", "metadata": {}}, {"source": "##Source Code\nhttps://github.com/juanjocarin/W207-final-project\n", "cell_type": "markdown", "metadata": {}}, {"source": "##Acknowledgements\n\nKaggle is hosting this competition for the machine learning community to use for fun and practice. \nThis dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine \nlearning repository for hosting the dataset. If you use the problem in publication, please cite:\n\nBache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California,\n    School of Information and Computer Science", "cell_type": "markdown", "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}